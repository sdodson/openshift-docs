= Upgrading OpenShift
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
In order to upgrade your OpenShift environment without disruption it is
important to upgrade components as documented in this guide. Please be sure to
familiarize yourself with the entire procedure before you begin your upgrade.
Specific versions may require additional steps outlined in version specific
section.

Unless noted otherwise, node and master versions within a major version are
forward and backward compatible so upgrading a cluster should go smoothly.
However you should not run mismatched versions longer than necessary to upgrade
the entire cluster.

[NOTE]
====
This documentation pertains to RPM based installations and does not currently
cover container based installations.
====

=== Masters
Upgrade your masters first and review master logs to ensure services have been
restarted successfully.

----
yum upgrade openshift-master
systemctl restart openshift-master
journalctl -r -u openshift-master
----

=== Update policy definitions
After a product update, the recommended default roles may be updated. To check
if an update is recommended for your environment, you can run `oadm policy
reconcile-cluster-roles`. This command will output a list of roles that are out
of date and their new values. You may either modify this output to re-apply
local policy changes you've made or automatically apply the new policy by running
`oadm policy reconcile-cluster-roles --confirm`.

----
# oadm policy reconcile-cluster-roles
apiVersion: v1
items:
- apiVersion: v1
  kind: ClusterRole
  metadata:
    creationTimestamp: null
    name: admin
  rules:
  - attributeRestrictions: null
    resources:
    - builds/custom
...
---

[NOTE] Your output will vary based on version and any local customizations you
have made. Please review the proposed policy carefully.

---
# oadm policy reconcile-cluster-roles --confirm
----

=== Nodes
When restarting a node there will be a brief disruption of outbound network
connectivity from running pods to services while the kube proxy is restarted.
The length of this disruption should be very short and will scale based on the
number of services in the entire cluster.

----
yum upgrade openshift\*
sytemctl restart openshift-node
----

==== Verify that all nodes are showing as ready
----
oc get nodes
NAME                      LABELS                                           STATUS
ose3-master.example.com   kubernetes.io/hostname=ose3-master.example.com   Ready,SchedulingDisabled
ose3-node1.example.com    kubernetes.io/hostname=ose3-node1.example.com    Ready
ose3-node2.example.com    kubernetes.io/hostname=ose3-node2.example.com    Ready
----

=== Router
The router deployment config must be upgraded in order to apply updates contained
in the router image.
[IMPORTANT]
In order to upgrade your router without disrupting services you need to have
deployed a link:high_availability.html#configuring-a-highly-available-routing-service[Highly-available Routing Service]/

----
oc edit dc/router
----
Adjust the rollingParams if needed to allow `down-then-up` rolling upgrades. This
is necessary if the replica count is equal to the number of nodes that match the
selector for your router, ie: replicas=2 and you have specified 'zone=infra' with
only two nodes that match that value.
[TODO]
Need to update this after https://github.com/openshift/origin/pull/3701 merges
----
..
spec:
  replicas: 2
  selector:
    router: router
  strategy:
    resources: {}
    rollingParams:
      intervalSeconds: 1
      timeoutSeconds: 120
      updatePeriodSeconds: 1
    type: Rolling
..
----

Next update the image version to match the version you're upgrading to, ie: 'v3.0.1.0'

----
image: openshift3/ose-haproxy-router:v3.0.1.0
imagePullPolicy: IfNotPresent
----

You should see one router pod updated and then the next.

=== Registry
The registry must also be upgraded in order to realize changes in the registry
image. If you have used a `*PersistentVolumeClaim*` or a host mount point you
may restart the registry without losing the contents of your registry. The
link:install/docker_registry.html#storage-for-the-registry[registry installation]
documentation details how to configure persistent storage.

Set the image version according to the version you're upgrading to, ie: 'v3.0.1.0'

----
oc edit dc/docker-registry
----

----
image: openshift3/ose-docker-registry:v3.0.1.0
imagePullPolicy: IfNotPresent
----
[NOTE]
Images that are being pushed or pulled from the internal registry at the time of
upgrade will fail and should be restarted automatically. This will not disrupt
already running pods.

=== Updating ImageStreams
You may also wish to ensure that your database and S2I ImageStreams are updated.
For each image stream in your openshift namespace run `oc image-import`.
[NOTE]
This will trigger a rebuild of all images built from these image streams due to
the ImageChange trigger. Please ensure that all pods that persist data have
properly configured persistent volumes. Pods with replica count greater than  one
will be updated in a rolling manner.

----
# oc get is -n openshift
NAME     DOCKER REPO                                                      TAGS                   UPDATED
mongodb  registry.access.redhat.com/openshift3/mongodb-24-rhel7           2.4,latest,v3.0.0.0    16 hours ago
mysql    registry.access.redhat.com/openshift3/mysql-55-rhel7             5.5,latest,v3.0.0.0    16 hours ago
nodejs   registry.access.redhat.com/openshift3/nodejs-010-rhel7           0.10,latest,v3.0.0.0   16 hours ago
...

# oc import-image -n openshift nodejs
Waiting for the import to complete, CTRL+C to stop waiting.
The import completed successfully.

Name:                   nodejs
Created:                16 hours ago
Labels:                 <none>
Annotations:            openshift.io/image.dockerRepositoryCheck=2015-07-21T13:17:00Z
Docker Pull Spec:       registry.access.redhat.com/openshift3/nodejs-010-rhel7

Tag             Spec            Created         PullSpec                                                        Image
0.10            latest          16 hours ago    registry.access.redhat.com/openshift3/nodejs-010-rhel7:latest   66d92cebc0e48e4e4be3a93d0f9bd54f21af7928ceaa384d20800f6e6fcf669f
latest                          16 hours ago    registry.access.redhat.com/openshift3/nodejs-010-rhel7:latest   66d92cebc0e48e4e4be3a93d0f9bd54f21af7928ceaa384d20800f6e6fcf669f
v3.0.0.0        <pushed>        16 hours ago    registry.access.redhat.com/openshift3/nodejs-010-rhel7:v3.0.0.0 66d92cebc0e48e4e4be3a93d0f9bd54f21af7928ceaa384d20800f6e6fcf669f
----

== Version Specific Steps
ifdef::openshift-enterprise[]
=== OSE 3.0.0.0 to 3.0.1.0
Perhaps this should go in release notes
endif::[]
ifdef::openshift-origin[]
=== OpenShift Origin 1.0.0 to 1.0.1
Perhaps this should go in release notes
endif::[]

== TODO
* Once the installer allows you to add nodes, document adding additional capacity
then tearing down old nodes rather than upgrading nodes in place.
* Quantify the impact of kube proxy restarts on a moderately sized node, how
long are services busted, what happens etc.
